{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_WIFI_Location_Tracking_Insights_RSSI_Scatter .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKMlISmQa50KSOUaNw6l6G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akramIOT/IOT_BU/blob/master/ML_WIFI_Location_Tracking_Insights_RSSI_Scatter_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHPQAMV0tJi0",
        "colab_type": "code",
        "outputId": "a01a847e-73ba-4ce8-9539-69211a49ead6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "!git clone --branch r1.13.0 --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc4\n",
            "fatal: destination path 'models' already exists and is not an empty directory.\n",
            "Requirement already satisfied: scapy in /usr/local/lib/python3.6/dist-packages (2.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moltUI5X48Be",
        "colab_type": "code",
        "outputId": "f057745c-418d-45ed-f72a-ccb0c59cf431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## My environment is having Tensorflow 2.0 and Python 3.7 Installed, along with other frameworks.\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gym\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "dataset = pd.read_csv(\"/content/trainingData.csv\",header = 0)\n",
        "\n",
        "features = np.asarray(dataset.iloc[:,0:520])\n",
        "features[features == 100] = -110\n",
        "features = (features - features.mean()) / features.var()\n",
        "print(\"features_mean:\", features.mean())\n",
        "print(\"features:\", features.shape)\n",
        "\n",
        "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
        "print(\"Labels:\", labels)\n",
        "labels = np.asarray(pd.get_dummies(labels))\n",
        "\n",
        "train_val_split = np.random.rand(len(features)) < 0.70\n",
        "train_x = features[train_val_split]\n",
        "print(train_x)\n",
        "train_y = labels[train_val_split]\n",
        "val_x = features[~train_val_split]\n",
        "print(val_x)\n",
        "val_y = labels[~train_val_split]\n",
        "\n",
        "test_dataset = pd.read_csv(\"/content/validationData.csv\",header = 0)\n",
        "\n",
        "test_features = np.asarray(test_dataset.iloc[:,0:520])\n",
        "test_features[test_features == 100] = -110\n",
        "test_features = (test_features - test_features.mean()) / test_features.var()\n",
        "\n",
        "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
        "test_labels = np.asarray(pd.get_dummies(test_labels))\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.0, shape = shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "# Input and Hidden Layer details \n",
        "\n",
        "n_input = 520 \n",
        "n_hidden_1 = 256 \n",
        "n_hidden_2 = 128 \n",
        "n_hidden_3 = 64 \n",
        "\n",
        "n_classes = labels.shape[1]\n",
        "\n",
        "learning_rate = 0.00001\n",
        "training_epochs = 30\n",
        "batch_size = 15\n",
        "\n",
        "total_batches = train_x.shape[0] // batch_size\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,n_input])\n",
        "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
        "\n",
        "# --------------------- Encoder Variables --------------- #\n",
        "# Optimizing Bias and weights via  DNN is an important problem to solve\n",
        "\n",
        "e_weights_h1 = weight_variable([n_input, n_hidden_1])\n",
        "e_biases_h1 = bias_variable([n_hidden_1])\n",
        "\n",
        "e_weights_h2 = weight_variable([n_hidden_1, n_hidden_2])\n",
        "e_biases_h2 = bias_variable([n_hidden_2])\n",
        "\n",
        "e_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
        "e_biases_h3 = bias_variable([n_hidden_3])\n",
        "\n",
        "# --------------------- Decoder Variables --------------- #\n",
        "\n",
        "d_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
        "d_biases_h1 = bias_variable([n_hidden_2])\n",
        "\n",
        "d_weights_h2 = weight_variable([n_hidden_2, n_hidden_1])\n",
        "d_biases_h2 = bias_variable([n_hidden_1])\n",
        "\n",
        "d_weights_h3 = weight_variable([n_hidden_1, n_input])\n",
        "d_biases_h3 = bias_variable([n_input])\n",
        "\n",
        "# --------------------- DNN Variables ------------------ #\n",
        "\n",
        "dnn_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
        "dnn_biases_h1 = bias_variable([n_hidden_2])\n",
        "\n",
        "dnn_weights_h2 = weight_variable([n_hidden_2, n_hidden_2])\n",
        "dnn_biases_h2 = bias_variable([n_hidden_2])\n",
        "\n",
        "dnn_weights_out = weight_variable([n_hidden_2, n_classes])\n",
        "dnn_biases_out = bias_variable([n_classes])\n",
        "\n",
        "\n",
        "def encode(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,e_weights_h1),e_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,e_weights_h2),e_biases_h2))\n",
        "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,e_weights_h3),e_biases_h3))\n",
        "    return l3\n",
        "    \n",
        "def decode(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,d_weights_h1),d_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,d_weights_h2),d_biases_h2))\n",
        "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,d_weights_h3),d_biases_h3))\n",
        "    return l3\n",
        "\n",
        "def dnn(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,dnn_weights_h1),dnn_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,dnn_weights_h2),dnn_biases_h2))\n",
        "    out = tf.nn.softmax(tf.add(tf.matmul(l2,dnn_weights_out),dnn_biases_out))\n",
        "    return out\n",
        "\n",
        "encoded = encode(X)\n",
        "decoded = decode(encoded) \n",
        "y_ = dnn(encoded)\n",
        "\n",
        "us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
        "s_cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
        "us_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(us_cost_function)\n",
        "s_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(s_cost_function)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "with tf.Session() as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    \n",
        "    # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n",
        "    for epoch in range(training_epochs):\n",
        "        epoch_costs = np.empty(0)\n",
        "        for b in range(total_batches):\n",
        "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([us_optimizer, us_cost_function],feed_dict={X: batch_x})\n",
        "            epoch_costs = np.append(epoch_costs,c)\n",
        "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
        "    print(\"Unsupervised pre-training finished...\")\n",
        "    \n",
        "    \n",
        "    # ---------------- 2. Training NN - Supervised Learning ------------------ #\n",
        "    for epoch in range(training_epochs):\n",
        "        epoch_costs = np.empty(0)\n",
        "        for b in range(total_batches):\n",
        "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :]\n",
        "            batch_y = train_y[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
        "            epoch_costs = np.append(epoch_costs,c)\n",
        "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \", \\\n",
        "            session.run(accuracy, feed_dict={X: train_x, Y: train_y}), \\\n",
        "            \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
        "            \n",
        "    print(\"Supervised training finished...\")\n",
        "    \n",
        "\n",
        "    print(\"\\nTesting Accuracy:\", session.run(accuracy, feed_dict={X: test_features, Y: test_labels}))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features_mean: -2.735186944520825e-17\n",
            "features: (19937, 520)\n",
            "Labels: ['12' '12' '12' ... '13' '13' '13']\n",
            "[[-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]\n",
            " [-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]\n",
            " [-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]\n",
            " ...\n",
            " [-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]\n",
            " [-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]\n",
            " [-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]]\n",
            "[[-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]\n",
            " [-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]\n",
            " [-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]\n",
            " ...\n",
            " [-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]\n",
            " [-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]\n",
            " [-0.02819699 -0.02819699 -0.02819699 ... -0.02819699 -0.02819699\n",
            "  -0.02819699]]\n",
            "Epoch:  0  Loss:  0.062713915282523\n",
            "Epoch:  1  Loss:  0.04184148824171813\n",
            "Epoch:  2  Loss:  0.032313036813229994\n",
            "Epoch:  3  Loss:  0.02635685941863242\n",
            "Epoch:  4  Loss:  0.02234588884513941\n",
            "Epoch:  5  Loss:  0.01957826885681236\n",
            "Epoch:  6  Loss:  0.017503997188716296\n",
            "Epoch:  7  Loss:  0.01586541400060441\n",
            "Epoch:  8  Loss:  0.01455893046606935\n",
            "Epoch:  9  Loss:  0.013517157578419611\n",
            "Epoch:  10  Loss:  0.012674425688940564\n",
            "Epoch:  11  Loss:  0.0119767830278405\n",
            "Epoch:  12  Loss:  0.011386384669190227\n",
            "Epoch:  13  Loss:  0.010876062305919658\n",
            "Epoch:  14  Loss:  0.010426392608560774\n",
            "Epoch:  15  Loss:  0.010024139642643584\n",
            "Epoch:  16  Loss:  0.009660873644199673\n",
            "Epoch:  17  Loss:  0.009331504979529214\n",
            "Epoch:  18  Loss:  0.009032908653379253\n",
            "Epoch:  19  Loss:  0.008762927025173254\n",
            "Epoch:  20  Loss:  0.008519688412040025\n",
            "Epoch:  21  Loss:  0.008301127501109581\n",
            "Epoch:  22  Loss:  0.008104784694916517\n",
            "Epoch:  23  Loss:  0.00792791587944439\n",
            "Epoch:  24  Loss:  0.007767756783981415\n",
            "Epoch:  25  Loss:  0.007621777971601563\n",
            "Epoch:  26  Loss:  0.0074878497139549745\n",
            "Epoch:  27  Loss:  0.007364265642169811\n",
            "Epoch:  28  Loss:  0.007249656207470217\n",
            "Epoch:  29  Loss:  0.007142871518949764\n",
            "Unsupervised pre-training finished...\n",
            "Epoch:  0  Loss:  36.132580643681464  Training Accuracy:  0.43307143 Validation Accuracy: 0.43052047\n",
            "Epoch:  1  Loss:  29.621339164235074  Training Accuracy:  0.536 Validation Accuracy: 0.52737075\n",
            "Epoch:  2  Loss:  23.876366933186848  Training Accuracy:  0.65164286 Validation Accuracy: 0.6429173\n",
            "Epoch:  3  Loss:  19.547485504886346  Training Accuracy:  0.72642857 Validation Accuracy: 0.71618664\n",
            "Epoch:  4  Loss:  16.37357713069691  Training Accuracy:  0.78892857 Validation Accuracy: 0.78457135\n",
            "Epoch:  5  Loss:  13.8911411128499  Training Accuracy:  0.8471429 Validation Accuracy: 0.8351019\n",
            "Epoch:  6  Loss:  11.856608102048197  Training Accuracy:  0.876 Validation Accuracy: 0.86104095\n",
            "Epoch:  7  Loss:  10.164960812142425  Training Accuracy:  0.9039286 Validation Accuracy: 0.8925383\n",
            "Epoch:  8  Loss:  8.75128485573833  Training Accuracy:  0.92464286 Validation Accuracy: 0.9149402\n",
            "Epoch:  9  Loss:  7.562875522166842  Training Accuracy:  0.93957144 Validation Accuracy: 0.9292572\n",
            "Epoch:  10  Loss:  6.556904262157371  Training Accuracy:  0.9516429 Validation Accuracy: 0.94205827\n",
            "Epoch:  11  Loss:  5.699147509161427  Training Accuracy:  0.96185714 Validation Accuracy: 0.95620686\n",
            "Epoch:  12  Loss:  4.96270970190453  Training Accuracy:  0.96671426 Validation Accuracy: 0.9607546\n",
            "Epoch:  13  Loss:  4.327117690255532  Training Accuracy:  0.97092855 Validation Accuracy: 0.96597606\n",
            "Epoch:  14  Loss:  3.7771085459830807  Training Accuracy:  0.975 Validation Accuracy: 0.9706923\n",
            "Epoch:  15  Loss:  3.30116577773871  Training Accuracy:  0.97742856 Validation Accuracy: 0.9742294\n",
            "Epoch:  16  Loss:  2.8900939711356597  Training Accuracy:  0.9795 Validation Accuracy: 0.9760822\n",
            "Epoch:  17  Loss:  2.5359536839962518  Training Accuracy:  0.98235714 Validation Accuracy: 0.97877717\n",
            "Epoch:  18  Loss:  2.231525075365365  Training Accuracy:  0.984 Validation Accuracy: 0.98012465\n",
            "Epoch:  19  Loss:  1.970187101684838  Training Accuracy:  0.9855 Validation Accuracy: 0.9813037\n",
            "Epoch:  20  Loss:  1.7459624960683167  Training Accuracy:  0.9862857 Validation Accuracy: 0.9816406\n",
            "Epoch:  21  Loss:  1.553549932609417  Training Accuracy:  0.98714286 Validation Accuracy: 0.9828196\n",
            "Epoch:  22  Loss:  1.3883003646633634  Training Accuracy:  0.98807144 Validation Accuracy: 0.98433554\n",
            "Epoch:  23  Loss:  1.2461633138430463  Training Accuracy:  0.9885714 Validation Accuracy: 0.98467237\n",
            "Epoch:  24  Loss:  1.1236344402913496  Training Accuracy:  0.9894286 Validation Accuracy: 0.98585147\n",
            "Epoch:  25  Loss:  1.0177056561568543  Training Accuracy:  0.98985714 Validation Accuracy: 0.9870305\n",
            "Epoch:  26  Loss:  0.9258224010898752  Training Accuracy:  0.99064285 Validation Accuracy: 0.9880411\n",
            "Epoch:  27  Loss:  0.8458364439643081  Training Accuracy:  0.99128574 Validation Accuracy: 0.98820955\n",
            "Epoch:  28  Loss:  0.7759557075774555  Training Accuracy:  0.9915714 Validation Accuracy: 0.98888326\n",
            "Epoch:  29  Loss:  0.71468759051026  Training Accuracy:  0.9919286 Validation Accuracy: 0.98922014\n",
            "Supervised training finished...\n",
            "\n",
            "Testing Accuracy: 0.91629165\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}